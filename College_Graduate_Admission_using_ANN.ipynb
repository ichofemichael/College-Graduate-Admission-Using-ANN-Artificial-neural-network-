{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# College Graduate Admission using ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GJWJzPSGTVOB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jU0cg8i2Tl9d"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/MyDrive/data/Admission_Predict.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "7q0JiJiQTq1G",
    "outputId": "ced023c7-c7bf-488c-f4d7-d503ce1c8e0d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Serial No.  GRE Score  TOEFL Score  ...  CGPA  Research  Chance of Admit \n",
       "0           1        337          118  ...  9.65         1              0.92\n",
       "1           2        324          107  ...  8.87         1              0.76\n",
       "2           3        316          104  ...  8.00         1              0.72\n",
       "3           4        322          110  ...  8.67         1              0.80\n",
       "4           5        314          103  ...  8.21         0              0.65\n",
       "\n",
       "[5 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QYzRLiKnTtIF",
    "outputId": "8938252a-97db-4ffa-b403-81d4a81e8ab2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yD604oAhTvYt",
    "outputId": "8e56ba19-e5eb-4d7f-c6a4-8dbf23ee490a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Serial No.           0\n",
       "GRE Score            0\n",
       "TOEFL Score          0\n",
       "University Rating    0\n",
       "SOP                  0\n",
       "LOR                  0\n",
       "CGPA                 0\n",
       "Research             0\n",
       "Chance of Admit      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5mexb5ZLTxI7"
   },
   "outputs": [],
   "source": [
    "df.drop(\"Serial No.\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "21DNIW-eT1IJ"
   },
   "outputs": [],
   "source": [
    "# Independent and Dependent features\n",
    "X = df.iloc[:,0:-1].values\n",
    "y = df.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fbxKja12T4Zn"
   },
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XRJKggTUT7br"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fQUrzij6T9A7",
    "outputId": "e21facc6-975e-4ad3-9c7b-9662f5540fd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.5.0\n",
      "  Downloading tensorflow-2.5.0-cp37-cp37m-manylinux2010_x86_64.whl (454.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 454.3 MB 16 kB/s \n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (3.17.3)\n",
      "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.15.0)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting grpcio~=1.34.0\n",
      "  Downloading grpcio-1.34.1-cp37-cp37m-manylinux2014_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 42.8 MB/s \n",
      "\u001b[?25hCollecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n",
      "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 56.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.19.5)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (0.2.0)\n",
      "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (2.7.0)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (0.37.0)\n",
      "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (0.4.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.1.2)\n",
      "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (0.12.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (3.3.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.1.0)\n",
      "Collecting keras-nightly~=2.5.0.dev\n",
      "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 43.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (3.1.0)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow==2.5.0) (1.5.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.35.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (57.4.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (3.3.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (2.23.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.8.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0) (4.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (2021.10.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (1.24.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0) (3.1.1)\n",
      "Building wheels for collected packages: wrapt\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68729 sha256=ed3e8fc35feda0467a1d53df0a18962324385a7a301c39d1c787dbac3de954d5\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
      "Successfully built wrapt\n",
      "Installing collected packages: typing-extensions, grpcio, wrapt, tensorflow-estimator, keras-nightly, flatbuffers, tensorflow\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.2\n",
      "    Uninstalling typing-extensions-3.10.0.2:\n",
      "      Successfully uninstalled typing-extensions-3.10.0.2\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.42.0\n",
      "    Uninstalling grpcio-1.42.0:\n",
      "      Successfully uninstalled grpcio-1.42.0\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.13.3\n",
      "    Uninstalling wrapt-1.13.3:\n",
      "      Successfully uninstalled wrapt-1.13.3\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.7.0\n",
      "    Uninstalling tensorflow-estimator-2.7.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 2.0\n",
      "    Uninstalling flatbuffers-2.0:\n",
      "      Successfully uninstalled flatbuffers-2.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.7.0\n",
      "    Uninstalling tensorflow-2.7.0:\n",
      "      Successfully uninstalled tensorflow-2.7.0\n",
      "Successfully installed flatbuffers-1.12 grpcio-1.34.1 keras-nightly-2.5.0.dev2021032900 tensorflow-2.5.0 tensorflow-estimator-2.5.0 typing-extensions-3.7.4.3 wrapt-1.12.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "typing_extensions"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install tensorflow==2.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "sbn0OhAoT-iJ",
    "outputId": "3fa7783f-30db-4e50-ddcb-a73f1b0798e8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "j2s9nBCOUB9d"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6CUswc9iUGku"
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "hy0_cUR4UIOp"
   },
   "outputs": [],
   "source": [
    "model.add(Dense(3, activation = \"relu\", input_dim = X_train.shape[1]))\n",
    "model.add(Dense(1, activation = \"linear\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5h8MKrw1UKlg",
    "outputId": "37729c7c-fc2a-4f6f-95e0-36be234f7b85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 3)                 24        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 28\n",
      "Trainable params: 28\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "LCMnkTtHUMFO"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = \"Adam\", loss = \"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HHkKAHZWUNxR",
    "outputId": "b2603bbd-a8a5-4ead-ce6a-86fb7eef07c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "36/36 [==============================] - 0s 953us/step - loss: 1.0554\n",
      "Epoch 2/10\n",
      "36/36 [==============================] - 0s 976us/step - loss: 0.7986\n",
      "Epoch 3/10\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.6396\n",
      "Epoch 4/10\n",
      "36/36 [==============================] - 0s 988us/step - loss: 0.5348\n",
      "Epoch 5/10\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.4587\n",
      "Epoch 6/10\n",
      "36/36 [==============================] - 0s 907us/step - loss: 0.3952\n",
      "Epoch 7/10\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3448\n",
      "Epoch 8/10\n",
      "36/36 [==============================] - 0s 999us/step - loss: 0.3029\n",
      "Epoch 9/10\n",
      "36/36 [==============================] - 0s 995us/step - loss: 0.2672\n",
      "Epoch 10/10\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2362\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb2a035b310>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs = 10, batch_size = 10, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "zvnfo2UaUPEu"
   },
   "outputs": [],
   "source": [
    "# Lets predict on the test data\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09BR89E4UQwc",
    "outputId": "f508e72b-ec2f-4138-88d2-d0ecf4a5a0d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.0133986117557194"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zekosf06USID",
    "outputId": "3ed38c5a-b905-4841-a5db-45ae47477bc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.4882\n",
      "Epoch 2/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.4139\n",
      "Epoch 3/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3382\n",
      "Epoch 4/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2616\n",
      "Epoch 5/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.1831\n",
      "Epoch 6/100\n",
      "36/36 [==============================] - 0s 977us/step - loss: 0.1262\n",
      "Epoch 7/100\n",
      "36/36 [==============================] - 0s 948us/step - loss: 0.0966\n",
      "Epoch 8/100\n",
      "36/36 [==============================] - 0s 955us/step - loss: 0.0767\n",
      "Epoch 9/100\n",
      "36/36 [==============================] - 0s 912us/step - loss: 0.0613\n",
      "Epoch 10/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0474\n",
      "Epoch 11/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0354\n",
      "Epoch 12/100\n",
      "36/36 [==============================] - 0s 961us/step - loss: 0.0293\n",
      "Epoch 13/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0252\n",
      "Epoch 14/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0227\n",
      "Epoch 15/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0209\n",
      "Epoch 16/100\n",
      "36/36 [==============================] - 0s 939us/step - loss: 0.0194\n",
      "Epoch 17/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0182\n",
      "Epoch 18/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0171\n",
      "Epoch 19/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0163\n",
      "Epoch 20/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0156\n",
      "Epoch 21/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0149\n",
      "Epoch 22/100\n",
      "36/36 [==============================] - 0s 996us/step - loss: 0.0142\n",
      "Epoch 23/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0135\n",
      "Epoch 24/100\n",
      "36/36 [==============================] - 0s 941us/step - loss: 0.0128\n",
      "Epoch 25/100\n",
      "36/36 [==============================] - 0s 954us/step - loss: 0.0123\n",
      "Epoch 26/100\n",
      "36/36 [==============================] - 0s 945us/step - loss: 0.0118\n",
      "Epoch 27/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0114\n",
      "Epoch 28/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0110\n",
      "Epoch 29/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0106\n",
      "Epoch 30/100\n",
      "36/36 [==============================] - 0s 940us/step - loss: 0.0103\n",
      "Epoch 31/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0099\n",
      "Epoch 32/100\n",
      "36/36 [==============================] - 0s 940us/step - loss: 0.0097\n",
      "Epoch 33/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0096\n",
      "Epoch 34/100\n",
      "36/36 [==============================] - 0s 962us/step - loss: 0.0095\n",
      "Epoch 35/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0094\n",
      "Epoch 36/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0092\n",
      "Epoch 37/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0090\n",
      "Epoch 38/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0089\n",
      "Epoch 39/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0088\n",
      "Epoch 40/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0085\n",
      "Epoch 41/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0082\n",
      "Epoch 42/100\n",
      "36/36 [==============================] - 0s 999us/step - loss: 0.0081\n",
      "Epoch 43/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0081\n",
      "Epoch 44/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0079\n",
      "Epoch 45/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0077\n",
      "Epoch 46/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0077\n",
      "Epoch 47/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0076\n",
      "Epoch 48/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0075\n",
      "Epoch 49/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0073\n",
      "Epoch 50/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0072\n",
      "Epoch 51/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0072\n",
      "Epoch 52/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0071\n",
      "Epoch 53/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0070\n",
      "Epoch 54/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0070\n",
      "Epoch 55/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0069\n",
      "Epoch 56/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0069\n",
      "Epoch 57/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0068\n",
      "Epoch 58/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0068\n",
      "Epoch 59/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0066\n",
      "Epoch 60/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0066\n",
      "Epoch 61/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0064\n",
      "Epoch 62/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0064\n",
      "Epoch 63/100\n",
      "36/36 [==============================] - 0s 980us/step - loss: 0.0064\n",
      "Epoch 64/100\n",
      "36/36 [==============================] - 0s 989us/step - loss: 0.0063\n",
      "Epoch 65/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0062\n",
      "Epoch 66/100\n",
      "36/36 [==============================] - 0s 971us/step - loss: 0.0061\n",
      "Epoch 67/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0060\n",
      "Epoch 68/100\n",
      "36/36 [==============================] - 0s 994us/step - loss: 0.0059\n",
      "Epoch 69/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0058\n",
      "Epoch 70/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0058\n",
      "Epoch 71/100\n",
      "36/36 [==============================] - 0s 920us/step - loss: 0.0057\n",
      "Epoch 72/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0057\n",
      "Epoch 73/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0056\n",
      "Epoch 74/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0056\n",
      "Epoch 75/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0055\n",
      "Epoch 76/100\n",
      "36/36 [==============================] - 0s 991us/step - loss: 0.0055\n",
      "Epoch 77/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0054\n",
      "Epoch 78/100\n",
      "36/36 [==============================] - 0s 994us/step - loss: 0.0054\n",
      "Epoch 79/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0053\n",
      "Epoch 80/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0053\n",
      "Epoch 81/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0053\n",
      "Epoch 82/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0052\n",
      "Epoch 83/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0052\n",
      "Epoch 84/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0051\n",
      "Epoch 85/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0051\n",
      "Epoch 86/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0050\n",
      "Epoch 87/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0050\n",
      "Epoch 88/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0050\n",
      "Epoch 89/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0050\n",
      "Epoch 90/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0049\n",
      "Epoch 91/100\n",
      "36/36 [==============================] - 0s 989us/step - loss: 0.0049\n",
      "Epoch 92/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0049\n",
      "Epoch 93/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0048\n",
      "Epoch 94/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0049\n",
      "Epoch 95/100\n",
      "36/36 [==============================] - 0s 997us/step - loss: 0.0048\n",
      "Epoch 96/100\n",
      "36/36 [==============================] - 0s 976us/step - loss: 0.0047\n",
      "Epoch 97/100\n",
      "36/36 [==============================] - 0s 997us/step - loss: 0.0048\n",
      "Epoch 98/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0047\n",
      "Epoch 99/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0047\n",
      "Epoch 100/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb29dc6fcd0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.add(Dense(3, activation = \"relu\", input_dim = X_train.shape[1]))\n",
    "model.add(Dense(1, activation = \"linear\"))\n",
    "\n",
    "# Model compile\n",
    "model.compile(optimizer = \"Adam\", loss = \"mean_squared_error\")\n",
    "\n",
    "# Model Training\n",
    "model.fit(X_train, y_train, epochs = 100, batch_size = 10, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "XNisz3FDUT-g"
   },
   "outputs": [],
   "source": [
    "# Lets predict on the test data\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q1zNJBCxUXYT",
    "outputId": "a0aa2298-82c6-4b68-a863-4be2a13c445c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5936250202321888"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R2 score\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L1poBmM0UYyI",
    "outputId": "61e111e6-65e5-4f28-cc73-5d7d36a67b38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.6261\n",
      "Epoch 2/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3465\n",
      "Epoch 3/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1826\n",
      "Epoch 4/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0667\n",
      "Epoch 5/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0138\n",
      "Epoch 6/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0064\n",
      "Epoch 7/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0057\n",
      "Epoch 8/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0054\n",
      "Epoch 9/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0053\n",
      "Epoch 10/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0051\n",
      "Epoch 11/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0051\n",
      "Epoch 12/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0050\n",
      "Epoch 13/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0050\n",
      "Epoch 14/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0049\n",
      "Epoch 15/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0049\n",
      "Epoch 16/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0048\n",
      "Epoch 17/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0048\n",
      "Epoch 18/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0047\n",
      "Epoch 19/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0047\n",
      "Epoch 20/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0046\n",
      "Epoch 21/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0046\n",
      "Epoch 22/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0045\n",
      "Epoch 23/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0045\n",
      "Epoch 24/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0044\n",
      "Epoch 25/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0044\n",
      "Epoch 26/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0044\n",
      "Epoch 27/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0043\n",
      "Epoch 28/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0043\n",
      "Epoch 29/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0043\n",
      "Epoch 30/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0042\n",
      "Epoch 31/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0041\n",
      "Epoch 32/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0041\n",
      "Epoch 33/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0041\n",
      "Epoch 34/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0041\n",
      "Epoch 35/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0040\n",
      "Epoch 36/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0039\n",
      "Epoch 37/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 38/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0039\n",
      "Epoch 39/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0039\n",
      "Epoch 40/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0039\n",
      "Epoch 41/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0039\n",
      "Epoch 42/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0039\n",
      "Epoch 43/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0039\n",
      "Epoch 44/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0039\n",
      "Epoch 45/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0039\n",
      "Epoch 46/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0039\n",
      "Epoch 47/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0038\n",
      "Epoch 48/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0038\n",
      "Epoch 49/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0037\n",
      "Epoch 50/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0038\n",
      "Epoch 51/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0038\n",
      "Epoch 52/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0037\n",
      "Epoch 53/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0038\n",
      "Epoch 54/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0037\n",
      "Epoch 55/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0037\n",
      "Epoch 56/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0038\n",
      "Epoch 57/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 58/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0038\n",
      "Epoch 59/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 60/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0037\n",
      "Epoch 61/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 62/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0037\n",
      "Epoch 63/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 64/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 65/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0036\n",
      "Epoch 66/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0037\n",
      "Epoch 67/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0036\n",
      "Epoch 68/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0037\n",
      "Epoch 69/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0037\n",
      "Epoch 70/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0037\n",
      "Epoch 71/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0037\n",
      "Epoch 72/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0037\n",
      "Epoch 73/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0038\n",
      "Epoch 74/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0037\n",
      "Epoch 75/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0036\n",
      "Epoch 76/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0038\n",
      "Epoch 77/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0035\n",
      "Epoch 78/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0036\n",
      "Epoch 79/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0037\n",
      "Epoch 80/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0036\n",
      "Epoch 81/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0036\n",
      "Epoch 82/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0035\n",
      "Epoch 83/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0036\n",
      "Epoch 84/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0036\n",
      "Epoch 85/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0036\n",
      "Epoch 86/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0036\n",
      "Epoch 87/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0036\n",
      "Epoch 88/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0036\n",
      "Epoch 89/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0035\n",
      "Epoch 90/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0035\n",
      "Epoch 91/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0036\n",
      "Epoch 92/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0036\n",
      "Epoch 93/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0036\n",
      "Epoch 94/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0036\n",
      "Epoch 95/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0036\n",
      "Epoch 96/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0035\n",
      "Epoch 97/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0035\n",
      "Epoch 98/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0036\n",
      "Epoch 99/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0035\n",
      "Epoch 100/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb29c31ec50>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.add(Dense(7, activation = \"relu\", input_dim = X_train.shape[1]))\n",
    "model.add(Dense(7, activation = \"relu\"))\n",
    "model.add(Dense(1, activation = \"linear\"))\n",
    "\n",
    "# Model compile\n",
    "model.compile(optimizer = \"Adam\", loss = \"mean_squared_error\")\n",
    "\n",
    "# Model Training\n",
    "model.fit(X_train, y_train, epochs = 100, batch_size = 10, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "lKoCQJeFUa6d"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0yXIKJtqUed6",
    "outputId": "dc5bcc9e-0db4-47d3-c63c-8fc13cec2782"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6477831445099369"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CeRq6CUCUf2Y",
    "outputId": "e8b4e7d9-6578-4c8c-d66d-dc22351a8d8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "29/29 [==============================] - 1s 8ms/step - loss: 0.5341 - val_loss: 0.4312\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.3474 - val_loss: 0.2629\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.1591 - val_loss: 0.0513\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0147 - val_loss: 0.0078\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.0042\n",
      "Epoch 6/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0041\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0037 - val_loss: 0.0040\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0041\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0039\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0036 - val_loss: 0.0039\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0035 - val_loss: 0.0039\n",
      "Epoch 12/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0038\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0039\n",
      "Epoch 14/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0039\n",
      "Epoch 15/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0035 - val_loss: 0.0038\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0038\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0038\n",
      "Epoch 18/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0035 - val_loss: 0.0040\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0038\n",
      "Epoch 20/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0038\n",
      "Epoch 21/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0035 - val_loss: 0.0041\n",
      "Epoch 22/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0038\n",
      "Epoch 23/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0038\n",
      "Epoch 24/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0035 - val_loss: 0.0039\n",
      "Epoch 25/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0038\n",
      "Epoch 26/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0034 - val_loss: 0.0039\n",
      "Epoch 27/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0038\n",
      "Epoch 28/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0039\n",
      "Epoch 29/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0034 - val_loss: 0.0038\n",
      "Epoch 30/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0035 - val_loss: 0.0038\n",
      "Epoch 31/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0035 - val_loss: 0.0038\n",
      "Epoch 32/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0039\n",
      "Epoch 33/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0041\n",
      "Epoch 34/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0038\n",
      "Epoch 35/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0039\n",
      "Epoch 36/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0038\n",
      "Epoch 37/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0039\n",
      "Epoch 38/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0039\n",
      "Epoch 39/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0035 - val_loss: 0.0039\n",
      "Epoch 40/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0034 - val_loss: 0.0038\n",
      "Epoch 41/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0038\n",
      "Epoch 42/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0042\n",
      "Epoch 43/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0036 - val_loss: 0.0040\n",
      "Epoch 44/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0041\n",
      "Epoch 45/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0035 - val_loss: 0.0039\n",
      "Epoch 46/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0040\n",
      "Epoch 47/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0039\n",
      "Epoch 48/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0039\n",
      "Epoch 49/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0035 - val_loss: 0.0040\n",
      "Epoch 50/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0038\n",
      "Epoch 51/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0039\n",
      "Epoch 52/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0041\n",
      "Epoch 53/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0034 - val_loss: 0.0039\n",
      "Epoch 54/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0038\n",
      "Epoch 55/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0038\n",
      "Epoch 56/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0035 - val_loss: 0.0039\n",
      "Epoch 57/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0040\n",
      "Epoch 58/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0039\n",
      "Epoch 59/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0040\n",
      "Epoch 60/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0039\n",
      "Epoch 61/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0038\n",
      "Epoch 62/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0034 - val_loss: 0.0039\n",
      "Epoch 63/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0040\n",
      "Epoch 64/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 65/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0039\n",
      "Epoch 66/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0035 - val_loss: 0.0040\n",
      "Epoch 67/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0039\n",
      "Epoch 68/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0039\n",
      "Epoch 69/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0039\n",
      "Epoch 70/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0039\n",
      "Epoch 71/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0039\n",
      "Epoch 72/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0039\n",
      "Epoch 73/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0044\n",
      "Epoch 74/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0042\n",
      "Epoch 75/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 76/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0034 - val_loss: 0.0040\n",
      "Epoch 77/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0040\n",
      "Epoch 78/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0040\n",
      "Epoch 79/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0034 - val_loss: 0.0040\n",
      "Epoch 80/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0039\n",
      "Epoch 81/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0043\n",
      "Epoch 82/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0035 - val_loss: 0.0039\n",
      "Epoch 83/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0040\n",
      "Epoch 84/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0034 - val_loss: 0.0040\n",
      "Epoch 85/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0034 - val_loss: 0.0040\n",
      "Epoch 86/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0034 - val_loss: 0.0041\n",
      "Epoch 87/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0040\n",
      "Epoch 88/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 89/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0041\n",
      "Epoch 90/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0044\n",
      "Epoch 91/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0039\n",
      "Epoch 92/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0040\n",
      "Epoch 93/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0041\n",
      "Epoch 94/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0040\n",
      "Epoch 95/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0034 - val_loss: 0.0040\n",
      "Epoch 96/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0040\n",
      "Epoch 97/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0041\n",
      "Epoch 98/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0041\n",
      "Epoch 99/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 100/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0041\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(7, activation = \"relu\", input_dim = X_train.shape[1]))\n",
    "model.add(Dense(7, activation = \"relu\"))\n",
    "model.add(Dense(1, activation = \"linear\"))\n",
    "\n",
    "# Model compile\n",
    "model.compile(optimizer = \"Adam\", loss = \"mean_squared_error\")\n",
    "\n",
    "# Model Training\n",
    "history = model.fit(X_train, y_train, epochs = 100, batch_size = 10, verbose = 1, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "QeqVzx5dUh-c"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eWxEGvy9Umhp",
    "outputId": "290e21ec-bff9-4f6c-f3b9-d78f4878ed6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6247256420780007"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "gbNJCRb0Un4z",
    "outputId": "c1cb63f5-3478-49ba-9516-0cee2de5d659"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb29a85bc50>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAZXElEQVR4nO3df4wc533f8fdnZn/c7d2RFK2DpZCSySRsG9pR/OOquK3rGo5TSEkhpo2DSGhRG3BLBI0Qt0nQykgrtOo/TVy4TVHBiKo4sY0miqMGDZuyFRLHRdE/7PAUC7IlWTYtyxIFOaItWxTJu/013/4xs8e94524pO64nJnPCzjwdna488w+c5977juz8ygiMDOz8kum3QAzM9seDnQzs4pwoJuZVYQD3cysIhzoZmYV0ZjWhq+//vo4cODAtDZvZlZKjz766LcjYnGz56YW6AcOHGB5eXlamzczKyVJ39zqOZdczMwqwoFuZlYRDnQzs4pwoJuZVYQD3cysIhzoZmYV4UA3M6uI0gX6iWdf5qOPfIVh5tv+mpmNK12gP/bc97j/c1/nfG8w7aaYmV1TShfos60UgJXecMotMTO7tpQu0OfaeaCfd6Cbma1TukCfbea3nznnkouZ2TqlC/SOSy5mZpsqXaC75GJmtrnSBfqo5OKrXMzM1itdoI9KLh6hm5mtV75Ad8nFzGxT5Qv0lksuZmabmSjQJd0m6WlJJyXds8nzH5R0WtJjxdc/2v6m5mabHqGbmW3mknOKSkqB+4EfB04BJyQdi4gnN6z6exFx9w60cZ00ETPNxJctmpltMMkI/VbgZEQ8ExE94CHgyM4267V1Wg2P0M3MNpgk0PcBz489PlUs2+inJT0u6WFJN232QpKOSlqWtHz69OkraG5utpn6k6JmZhts10nR/wEciIhbgD8GPrnZShHxQEQsRcTS4uLiFW+s00pdcjEz22CSQH8BGB9x7y+WrYmI70REt3j4IPCO7Wne5jptl1zMzDaaJNBPAIckHZTUAu4Ejo2vIOnGsYd3AE9tXxMv1mmmvmzRzGyDS17lEhEDSXcDjwAp8ImIeELSfcByRBwDfkHSHcAAeBn44A62mU4r5Vtn+ju5CTOz0rlkoANExHHg+IZl9459/xHgI9vbtK112g3X0M3MNijdJ0UhL7n4Khczs/VKGeizrdQnRc3MNihloM+188sWI2LaTTEzu2aUMtA7rQaDLOgNs2k3xczsmlHKQB/doMsnRs3MLihloHsaOjOzi5Uy0Gd9T3Qzs4uUMtA7vie6mdlFyhnonlfUzOwi5Qz0tksuZmYblTPQPUI3M7tIKQPd84qamV2slIE+Nyq5dF1yMTMbKWWgr5Vc+h6hm5mNlDLQ240EyZ8UNTMbV8pAl8Rcq8G5rgPdzGyklIEO+S10V/quoZuZjZQ20Du+J7qZ2ToTTUF3TTn3bXjl+XzWIpdczMzWlG+E/sVPwwPvYU9r4JKLmdmY8gV6ax6AvY2eSy5mZmNKG+h70h7nXXIxM1tTvkBvF4He6HLeJRczszXlC/RihL4r6fqDRWZmY0od6K6hm5ldUL5AL0ou80WgZ1lMuUFmZteG8gV6aw6Aea0AsDrwKN3MDEoZ6PkIfY5VwPdENzMbmSjQJd0m6WlJJyXd8xrr/bSkkLS0fU3coAj0ThSB7ksXzcyACQJdUgrcD9wOHAbuknR4k/UWgA8DX9juRq7TaEHaYpa85OJLF83McpOM0G8FTkbEMxHRAx4Cjmyy3r8FfhWKWshOas3RzopAd8nFzAyYLND3Ac+PPT5VLFsj6e3ATRHxP1/rhSQdlbQsafn06dOX3dg1rQVmsvOASy5mZiOv+6SopAT4GPBLl1o3Ih6IiKWIWFpcXLzyjbbmaA5HI3SXXMzMYLJAfwG4aezx/mLZyALwFuD/SHoWeCdwbEdPjLbnaQ7PAbDieUXNzIDJAv0EcEjSQUkt4E7g2OjJiHglIq6PiAMRcQD4PHBHRCzvSIsBWvM0BnnJxfdENzPLXTLQI2IA3A08AjwFfCYinpB0n6Q7drqBm2rNkfbzEbpLLmZmuYlmLIqI48DxDcvu3WLd97z+Zl1Ce4FkUJRcfJWLmRlQxk+KArTmUPcszVScc6CbmQGlDfR56J2l02qw4pKLmRlQ5kAf9tjVzPzBIjOzQjkDvbiF7t5W34FuZlYoZ6AXt9B9Q6Pvq1zMzAolDfSxeUU9QjczA8oa6O0FAPY0eg50M7NCOQO9KLnsSXsuuZiZFUoa6HnJZSHp+oNFZmaFkgZ6PkLflaz6g0VmZoVyBnpRQ5/XqkfoZmaFcgZ6MUKfY5XeMKM/zKbcIDOz6StnoDc7gJjD09CZmY2UM9AlaM0zU0xf6rKLmVlZAx2gPc9s5mnozMxGyhvorTna4ZKLmdlIiQN9nvYwn4bOgW5mVuZAby+sTRTtkouZWZkDvTVHc5iXXHxS1Mys1IE+PzZRtAPdzKzEgT63NlG0Sy5mZmUO9PYCSc8jdDOzkfIGemsO9c8hPK+omRmUOtCLeUWbfVb6DnQzsxIHejGvaHPgGrqZGWUO9OIWunsbXc53PUI3MytvoBcj9L1NzytqZgalDvS8hr6n0ee8a+hmZpMFuqTbJD0t6aSkezZ5/uckfUnSY5L+n6TD29/UDdpFoKerrLiGbmZ26UCXlAL3A7cDh4G7Ngns34mIH46ItwK/Bnxs21u6UTFC3530OOcaupnZRCP0W4GTEfFMRPSAh4Aj4ytExJmxh3NAbF8Tt1AE+kKy6ssWzcyAxgTr7AOeH3t8CvjRjStJ+nngF4EW8N7NXkjSUeAowM0333y5bV2vOCm6kHR92aKZGdt4UjQi7o+IHwD+BfAvt1jngYhYioilxcXF17fBYoQ+r1Vf5WJmxmSB/gJw09jj/cWyrTwE/NTradRE0gY0ZphjhZXekIidr/KYmV3LJgn0E8AhSQcltYA7gWPjK0g6NPbwJ4GvbV8TX0NrnllWGWRBb5hdlU2amV2rLllDj4iBpLuBR4AU+EREPCHpPmA5Io4Bd0t6H9AHvgt8YCcbvaY1x2xcmOSi3UivymbNzK5Fk5wUJSKOA8c3LLt37PsPb3O7JtNeYGZsoug9nam0wszsmlDeT4qCJ4o2MxtT8kCfo5WNRui+dNHM6q3cgd6ep+kRupkZUPZAb83TKOYVXXGgm1nNlT7Q075H6GZmUPpAnyPpnwXCNXQzq71yB3p7HsWQNn2P0M2s9sod6MX9XObw/VzMzEoe6PkdF+fkSS7MzMod6M38o6F7mkOP0M2s9ioS6H3OOdDNrOZKHuizAOxuDFxyMbPaK3mg5yP03enAJRczq72SB3o+Qt/V6HteUTOrvUoE+kLq69DNzEoe6HnJZT7pc67rGrqZ1VvJAz0foc8nPZdczKz2Sh7o+Qh9Lu255GJmtVfuQE+boIQOfd8+18xqr9yBLkGzw6y6nOsNiIhpt8jMbGrKHegAzVlm6BEB3UE27daYmU1NRQK9C3iSCzOrtwoEeod2jALdly6aWX1VINBnacUq4HlFzazeKhDoHVpZPkL3HRfNrM4qEOizNLJ8hO6Si5nVWTUCfeiSi5lZBQK9sxbovsrFzOqsAoE+SzJcAVxyMbN6myjQJd0m6WlJJyXds8nzvyjpSUmPS/qspDdtf1O30OyQDEaB7hG6mdXXJQNdUgrcD9wOHAbuknR4w2pfBJYi4hbgYeDXtruhW2rOQn8FCAe6mdXaJCP0W4GTEfFMRPSAh4Aj4ytExOci4nzx8PPA/u1t5mtozqIY0tbQJ0XNrNYmCfR9wPNjj08Vy7byIeB/bfaEpKOSliUtnz59evJWvpbiFrp7W0OP0M2s1rb1pKikfwAsAR/d7PmIeCAiliJiaXFxcXs2Wkxysac58ElRM6u1xgTrvADcNPZ4f7FsHUnvA34F+FsRxc1VroZihH5dc+ARupnV2iQj9BPAIUkHJbWAO4Fj4ytIehvwG8AdEfHS9jfzNRQj9N0NB7qZ1dslAz0iBsDdwCPAU8BnIuIJSfdJuqNY7aPAPPD7kh6TdGyLl9t+xQh9d6PPSt8lFzOrr0lKLkTEceD4hmX3jn3/vm1u1+TWRuh9vtL1CN3M6qsSnxQFmE88r6iZ1VsFAj0vuSykfc675GJmNVaBQC9G6KlH6GZWbxUI9HyEPqeer3Ixs1qrQKDnI/ROEehZFlNukJnZdJQ/0Bt5oM+qB8DqwKN0M6un8gd62oC0xSz5h1NddjGzuip/oAM0Z5kpAt0nRs2srioS6B3a9AE45xt0mVlNVSTQZ2lHPq/oua4D3czqqSKB3qFV3ODx1VUHupnVU0UCfdaBbma1V5lAbw7zkosD3czqqiKB3iHN8kA/2+1PuTFmZtNRkUCfJRmsIHmEbmb1VZFA76D+CvPthgPdzGqrIoE+C/3zLDjQzazGKhToKyzMNHl11TV0M6unigR6pxihp5z1B4vMrKYqEuj5HReva2cuuZhZbVUk0PNJLva2hy65mFltNabdgG1RjND3Ngec7XqCCzOrp4oEej5C39MYcGbVgW5m9VSRkks+Qt/d7NMbZHQ9a5GZ1VClAn1Xmp8QPesTo2ZWQxUJ9LzkspDmJ0R9pYuZ1VFFAj0foc8n+UTRvhbdzOqoIoGej9DnikA/40sXzayGJgp0SbdJelrSSUn3bPL8uyX9uaSBpPdvfzMvoRihz8klFzOrr0sGuqQUuB+4HTgM3CXp8IbVngM+CPzOdjdwIsUIfVb5rEU+KWpmdTTJdei3Aicj4hkASQ8BR4AnRytExLPFc9kOtPHSihF6e20aOpdczKx+Jim57AOeH3t8qlh22SQdlbQsafn06dNX8hKba8wAMENeQ3fJxczq6KqeFI2IByJiKSKWFhcXt++FpXwauuEK7Ubiq1zMrJYmCfQXgJvGHu8vll1b1u6J3uCMR+hmVkOTBPoJ4JCkg5JawJ3AsZ1t1hVodtYmufAI3czq6JKBHhED4G7gEeAp4DMR8YSk+yTdASDpr0o6BfwM8BuSntjJRm+qmIYun1fUJ0XNrH4muttiRBwHjm9Ydu/Y9yfISzHTM1Zy8UlRM6ujanxSFC5MQzfT8HXoZlZLFQr0fIQ+3/ZE0WZWTxUK9I5LLmZWaxUK9Pyk6K6ZBmd7A7LMMxeZWb1ULNBXmJ9pEAHneh6lm1m9VCjQRydFm4DviW5m9VOhQL9w2SL4fi5mVj8VCvQODHss5AN0X+liZrVTnUAv7ri4qzkEPEI3s/qpTqAX90Tf5YmizaymKhTo+axFC6nviW5m9VShQM9H6PNJPkI/23UN3czqpTqBvnADADPnXkTyCN3M6qc6gf7GNwOQvPSl4ha6DnQzq5fqBPrMbrjuALz4OLtmmg50M6ud6gQ6wA23wLce9yQXZlZL1Qv0l59hsdXzR//NrHaqFeg33gLADyXPueRiZrVTrUC/IQ/0vxTfcMnFzGqnWoG+cAN0rufg4OsuuZhZ7VQr0CW48Rb2r36NMy65mFnNVCvQAW64hcWVbxCDHt3BcNqtMTO7aioY6D9MGn1+UC9w1qN0M6uR6gX6jT8CwJuTZ32li5nVSvUCfe/3M0xnOaxv+sSomdVK9QI9STm/94d4c/IsT754ZtqtMTO7aqoX6EDn5rfxlvQ57jv2Jb76F69OuzlmZldFJQM9/b5bmIvzvKP5HP/4U8t873xv2k0yM9txjUlWknQb8OtACjwYEf9uw/Nt4FPAO4DvAD8bEc9ub1Mvw8F3Q2OW3x7cw5+8usTHf+v93Pm338We2Ra7Oi3StAlJI/8CiCFEBkouLE9SQPm17RHFC0e+LElBab5+ZPkXMbYe+XNrX7qwfLROZJD1YdCFbJC/ZtrKv0b/D2DYg/4KDFbzdjVnoTELSbL+NbNh/jrEhbata/sG421e186xtkoX2r62jT4MR1+9vL3NmbxN0tj7wYb3L8aWF8+Nlo2Wj7/3o3ZnA+idhd65/L1Km5C2838jy5+PLG9Ho50/F8O8faP3NWle3NeRXdjGqN/T5oX3ZvTag27+NdpG2sy/lBbHQbL++Fjb1yjaNszfNyXr1x+9/2vv/VifjY6rbHBh/0avmw3zY2HQzddrdvJjImlc2N74e6l0fR9edJyOP8f65RHQfRVWvgvdM/myxkz+Ho8f00laLG+N/UzE2PEz2sbYfoz2OzLon8/7Nxvk+9NeKOYILvaXLY5hyJ9f19fFPmcDGHZh0MvbMPqZHfXpsJs/bs9Da+HC8TTe98N+vu2k6PNRJoz3dTa80Jej93F0jI2eG/V3khav08zvDtvqbL1fV0ix1Q/8aAUpBb4K/DhwCjgB3BURT46t80+AWyLi5yTdCfzdiPjZ13rdpaWlWF5efr3t39qZF+HEf6H7+Qdp91/Zue1MSVYcWMlrHezbZEhCSnbpFc1sIo/d8q9469/75Sv6v5IejYilzZ6bZIR+K3AyIp4pXuwh4Ajw5Ng6R4B/XXz/MPCfJSku9dtiJ+26EX7sXtp/85d57s/+kO+8/F3Odfuc6/bJsiHKBigbEEAWCRlCZCQxJIkBSfFbNyIrwlN5dEaQkJFEhsjISAiU/xJeG40EKtajeGVGryChYr2+GgxoMCBFkdGIPmn085AuRjJDNRkkbQZJizSGpFmXxnAVjQVskJAlKUGa/w0RAUUbAxXtomhDMcZQUozyg8jy/SSKPZVGe5W3OYJMCUNSMiWQtlDaJmk0SLI+6bBLOlwlCxgCgyh+2QRI+V81GQkIAqGItV9DGQmhpHjPMtKsX/y1lLc0U0IvmaWbzDJQi7aGtNSnGQP6JPQjYZhBIwak9GlmvaKtDTKlKIakDEmKkWuQENLav6CiP4ek0S/2OP81mSlloPy9B5HEgDTrkTJY6/+EbO2dGn+HQyKUkpES0lqfKIZAkr+P5L+Qk+J1WFsnI8u3wjDydmYIQgyTlIFaDNUiVdCKLu3o0mCw1j8RoMhIoo9i/S/iRAlK8mMwIsiyIBuNpgGh4kjPe2g1nWclnWclWSBR0Mx6NCIvYUbRj0kMaWRdGpGPaKM4erTW87HW11mxnRRoKCNI6CYzrGqWIQntWGUmO08zemSILMRw7SdooyBTg4yUodLivRySxpABKX016UcDJaJJRkNDMjXo06SnJsqGtLLztIbnaTAA8r+QQilD5cdPBKQxIGGQH0PB2rszOoYyEhIJKT/Gs7UeHe19/ropQ9IY0GDID7zpr2+6R6/XJIG+D3h+7PEp4Ee3WiciBpJeAd4AfHt8JUlHgaMAN9988xU2+TK1Otz8rru4SlszM5uaq3pSNCIeiIiliFhaXFy8mps2M6u8SQL9BeCmscf7i2WbriOpAewmPzlqZmZXySSBfgI4JOmgpBZwJ3BswzrHgA8U378f+NOp1s/NzGrokjX0oiZ+N/AI+bmMT0TEE5LuA5Yj4hjwm8CnJZ0EXiYPfTMzu4omug49Io4Dxzcsu3fs+1XgZ7a3aWZmdjkq+UlRM7M6cqCbmVWEA93MrCIu+dH/HduwdBr45hX+9+vZ8KGlmqjjftdxn6Ge+13HfYbL3+83RcSmH+SZWqC/HpKWt7qXQZXVcb/ruM9Qz/2u4z7D9u63Sy5mZhXhQDczq4iyBvoD027AlNRxv+u4z1DP/a7jPsM27ncpa+hmZnaxso7QzcxsAwe6mVlFlC7QJd0m6WlJJyXdM+327ARJN0n6nKQnJT0h6cPF8r2S/ljS14p/r5t2W7ebpFTSFyX9UfH4oKQvFP39e8UdPytF0h5JD0v6iqSnJP21mvT1PyuO7y9L+l1JM1Xrb0mfkPSSpC+PLdu0b5X7T8W+Py7p7Ze7vVIFejG/6f3A7cBh4C5Jh6fbqh0xAH4pIg4D7wR+vtjPe4DPRsQh4LPF46r5MPDU2ONfBf5DRPwg8F3gQ1Np1c76deB/R8RfAX6EfP8r3deS9gG/ACxFxFvI7+R6J9Xr798GbtuwbKu+vR04VHwdBT5+uRsrVaAzNr9pRPSA0fymlRIRL0bEnxffv0r+A76PfF8/Waz2SeCnptPCnSFpP/CTwIPFYwHvJZ+nFqq5z7uBd5PfgpqI6EXE96h4XxcawGwxKU4HeJGK9XdE/F/yW4qP26pvjwCfitzngT2Sbryc7ZUt0Deb33TflNpyVUg6ALwN+ALwxoh4sXjqW8Abp9SsnfIfgX8OazNgvwH4XkQMisdV7O+DwGngt4pS04OS5qh4X0fEC8C/B54jD/JXgEepfn/D1n37uvOtbIFeK5Lmgf8G/NOIODP+XDEjVGWuOZX0d4CXIuLRabflKmsAbwc+HhFvA86xobxStb4GKOrGR8h/oX0fMMfFpYnK2+6+LVugTzK/aSVIapKH+X+NiD8oFv/F6E+w4t+XptW+HfA3gDskPUteSnsveW15T/EnOVSzv08BpyLiC8Xjh8kDvsp9DfA+4BsRcToi+sAfkB8DVe9v2LpvX3e+lS3QJ5nftPSK2vFvAk9FxMfGnhqfu/UDwB9e7bbtlIj4SETsj4gD5P36pxHx94HPkc9TCxXbZ4CI+BbwvKS/XCz6MeBJKtzXheeAd0rqFMf7aL8r3d+Frfr2GPAPi6td3gm8MlaamUxElOoL+Angq8DXgV+Zdnt2aB/fRf5n2OPAY8XXT5DXlD8LfA34E2DvtNu6Q/v/HuCPiu+/H/gz4CTw+0B72u3bgf19K7Bc9Pd/B66rQ18D/wb4CvBl4NNAu2r9Dfwu+TmCPvlfYx/aqm8BkV/F93XgS+RXAF3W9vzRfzOziihbycXMzLbgQDczqwgHuplZRTjQzcwqwoFuZlYRDnQzs4pwoJuZVcT/B9hbG8Sz7waaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1KuMtLemUq4Y"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "College Graduate Admission using ANN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
